{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a = tf.constant([2,3], dtype= tf.int32)\n",
    "a = tf.range(1,10,delta=2)\n",
    "a = tf.linspace(0,10,20)\n",
    "a = tf.eye(3,3)\n",
    "a = tf.linalg.diag(np.arange(12).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.slice(input,begin_vector,size_vector)\n",
    "tf.slice(tf.reshape(tf.range(12),[3,4]),[1,0],[2,2]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for assign operator\n",
    "# first we need to set a variable\n",
    "a = tf.Variable(np.zeros([4,4]), dtype= tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(4, 4) dtype=float16, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float16)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:2].assign(tf.ones([2,4],dtype=tf.float16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#省略号可以表示多个冒号\n",
    "tf.print(a[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = tf.random.uniform((4,10,7),minval=0,maxval=100,dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.gather(scores,[0,5,9],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩\n",
    "q = tf.gather(tf.gather(scores,[0,5,9],axis=1),[1,3,6],axis=2)\n",
    "tf.print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[74, 37, 10, 92, 54, 58,  0],\n",
       "       [37, 68, 52, 55, 45, 42, 60],\n",
       "       [11, 11, 62, 97, 12, 76, 83]])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "#indices的长度为采样样本的个数，每个元素为采样位置的坐标\n",
    "tf.gather_nd(scores,indices = [(0,0),(2,4),(3,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([74, 68, 62])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather_nd(scores,indices = [(0,0,0),(2,4,1),(3,6,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.boolean_mask(scores,[True,False,False,False,False,\n",
    "                            True,False,False,False,True],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "s = tf.boolean_mask(scores,\n",
    "    [[True,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,True,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,True,False,False,False]])\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.print(tf.boolean_mask(c,c<0),\"\\n\") \n",
    "tf.print(c[c<0]) #布尔索引，为boolean_mask的语法糖形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。\n",
    "\n",
    "如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。\n",
    "\n",
    "tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。\n",
    "\n",
    "tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，\n",
    "\n",
    "而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[nan,  1., nan],\n",
       "       [ 2.,  2., nan],\n",
       "       [ 3., nan,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找到张量中小于0的元素,将其换成np.nan得到新的张量\n",
    "#tf.where和np.where作用类似，可以理解为if的张量版本\n",
    "\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "d = tf.where(c<0,tf.fill(c.shape,np.nan),c) \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
       "array([[0, 0],\n",
       "       [0, 2],\n",
       "       [1, 2],\n",
       "       [2, 1]], dtype=int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果where只有一个参数，将返回所有满足条件的位置坐标\n",
    "indices = tf.where(c<0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[nan,  1., nan],\n",
       "       [ 1.,  1., nan],\n",
       "       [ 1., nan,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "d = tf.where(c<0,tf.fill(c.shape,np.nan),tf.fill(c.shape,1.0)) \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.,  1., -1.],\n",
       "       [ 2.,  2., -2.],\n",
       "       [ 3.,  0.,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量\n",
    "d = c - tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0., -3.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[5, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 5, 0]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.scatter_nd([[0,0],[2,1]],[5,5],c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_nd的作用和gather_nd有些相反\n",
    "#可以将某些值插入到一个给定shape的全0的张量的指定位置处。\n",
    "indices = tf.where(c<0)\n",
    "tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape 可以改变张量的形状。\n",
    "\n",
    "tf.squeeze 可以减少维度。\n",
    "\n",
    "tf.expand_dims 可以增加维度。\n",
    "\n",
    "tf.transpose 可以交换维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-1.,  1., -1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 2.,  2., -2.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 3., -3.,  3.]], dtype=float32)>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.split(value,num_or_size_splits,axis)\n",
    "tf.split(c,3,axis = 0)  #指定分割份数，平均分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
       " array([[0.8935895 , 0.895692  , 0.91402435, 0.69897413, 0.7914586 ,\n",
       "         0.3803165 , 0.2065692 , 0.21990776, 0.5720501 , 0.3484335 ,\n",
       "         0.3429693 , 0.68787324]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
       " array([[0.6472976 , 0.82212925, 0.11641526, 0.5760038 , 0.7230698 ,\n",
       "         0.9944571 , 0.31512165, 0.85974634, 0.4827547 , 0.67862284,\n",
       "         0.92354727, 0.14661741]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 12), dtype=float32, numpy=\n",
       " array([[0.99819493, 0.5212045 , 0.88263226, 0.8542752 , 0.57050717,\n",
       "         0.42869663, 0.03776813, 0.7298223 , 0.8275517 , 0.70772684,\n",
       "         0.5436336 , 0.26363873],\n",
       "        [0.53545046, 0.7393999 , 0.18041193, 0.3841871 , 0.30740607,\n",
       "         0.28331494, 0.2041887 , 0.6020137 , 0.8725177 , 0.28952563,\n",
       "         0.00612175, 0.7088337 ],\n",
       "        [0.13641608, 0.81328964, 0.39551198, 0.66780996, 0.80712867,\n",
       "         0.6830174 , 0.40002465, 0.9775413 , 0.28007185, 0.38232505,\n",
       "         0.7020668 , 0.6807004 ],\n",
       "        [0.44333112, 0.23620105, 0.14678717, 0.5210991 , 0.5567007 ,\n",
       "         0.73022544, 0.08885598, 0.22472191, 0.11356485, 0.10325015,\n",
       "         0.6594106 , 0.89505994],\n",
       "        [0.44322717, 0.4019519 , 0.81585956, 0.9228406 , 0.6221237 ,\n",
       "         0.57156444, 0.4641291 , 0.40936136, 0.58286476, 0.34387612,\n",
       "         0.23966718, 0.42006886],\n",
       "        [0.9246521 , 0.6850989 , 0.20096946, 0.6476792 , 0.94746447,\n",
       "         0.09912741, 0.8633046 , 0.80559886, 0.16056752, 0.01706541,\n",
       "         0.94530964, 0.3029269 ],\n",
       "        [0.88710845, 0.8360592 , 0.87709546, 0.9970552 , 0.48986375,\n",
       "         0.00106692, 0.49685502, 0.53477395, 0.4033358 , 0.7689842 ,\n",
       "         0.8318987 , 0.06371641],\n",
       "        [0.8733219 , 0.32999325, 0.1358037 , 0.7478365 , 0.70289373,\n",
       "         0.20315015, 0.06183314, 0.16557884, 0.7845888 , 0.73479307,\n",
       "         0.74809265, 0.97706187],\n",
       "        [0.67037654, 0.68607736, 0.1156615 , 0.41277254, 0.3027476 ,\n",
       "         0.13272989, 0.8483639 , 0.5048162 , 0.35739768, 0.72874403,\n",
       "         0.03073215, 0.41928327],\n",
       "        [0.05031264, 0.09355462, 0.31964016, 0.88444364, 0.9986942 ,\n",
       "         0.22545898, 0.18786943, 0.80729175, 0.7088314 , 0.25047028,\n",
       "         0.20663488, 0.47324085]], dtype=float32)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.random.uniform([12,12])\n",
    "tf.split(c,[1,1,-1],axis = 0) #指定每份的记录数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 -0.8 1 -1 0.7]\n",
      "[0.0264732055 -0.0235317405 2.94146752 -0.588293493 0.0205902718]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([0.9,-0.8,100.0,-20.0,0.7])\n",
    "y = tf.clip_by_value(x,clip_value_min=-1,clip_value_max=1)\n",
    "z = tf.clip_by_norm(x,clip_norm = 3)\n",
    "tf.print(y)\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([ 0.02647321, -0.02353174,  2.9414675 , -0.5882935 ,  0.02059027],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/tf.norm(x)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。 许多向量运算符都以reduce开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量reduce\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.reduce_sum(a))\n",
    "tf.print(tf.reduce_mean(a))\n",
    "tf.print(tf.reduce_max(a))\n",
    "tf.print(tf.reduce_min(a))\n",
    "tf.print(tf.reduce_prod(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(tf.range(1,5),[2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 8]\r\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_prod(a,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#张量指定维度进行reduce\n",
    "b = tf.reshape(a,(3,3))\n",
    "tf.print(tf.reduce_sum(b, axis=1, keepdims=True))\n",
    "tf.print(tf.reduce_sum(b, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bool类型的reduce\n",
    "p = tf.constant([True,False,False])\n",
    "q = tf.constant([False,False,True])\n",
    "tf.print(tf.reduce_all(p))\n",
    "tf.print(tf.reduce_any(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 66 72 78]\r\n"
     ]
    }
   ],
   "source": [
    "c = tf.reshape(tf.range(24),[6,4])\n",
    "#利用tf.foldr实现tf.reduce_sum\n",
    "s = tf.foldr(lambda a,b:a+b,c) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 66 72 78]\r\n"
     ]
    }
   ],
   "source": [
    "c = tf.reshape(tf.range(24),[6,4])\n",
    "#利用tf.foldr实现tf.reduce_sum\n",
    "s = tf.foldl(lambda a,b:a+b,c) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "至于tf.foldr()和这个函数是基本上一样的，无非就是从右边开始计算到左边而已。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cum扫描累积\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.math.cumsum(a))\n",
    "tf.print(tf.math.cumprod(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 8 7]\n",
      "[3 5 2]\n"
     ]
    }
   ],
   "source": [
    "#tf.math.top_k可以用于对张量排序\n",
    "a = tf.constant([1,3,7,5,4,8])\n",
    "\n",
    "values,indices = tf.math.top_k(a,3,sorted=False)\n",
    "tf.print(values)\n",
    "tf.print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵乘法\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[2,0],[0,2]])\n",
    "a@b  #等价于tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.transpose/ tf.linalg.inv/ tf.linalg.trace/ tf.linalg.eigvals/ tf.linalg.det\n",
    "tf.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#broadcast\n",
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])\n",
    "b + a  # Identical to b + tf.broadcast_to(a,b.shape)\n",
    "tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "一，Autograph编码规范总结\n",
    "1，被@tf.function修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print，使用tf.range而不是range，使用tf.constant(True)而不是True.\n",
    "\n",
    "2，避免在@tf.function修饰的函数内部定义tf.Variable.\n",
    "\n",
    "3，被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "def myadd(a,b):\n",
    "    for i in tf.range(3):\n",
    "        tf.print(i)\n",
    "    c = a+b\n",
    "    print(\"tracing\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "tracing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'heloak'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(\"helo\"), tf.constant(\"ak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "@tf.function\n",
    "def myadd(a,b):\n",
    "    for i in tf.range(3):\n",
    "        tf.print(i)\n",
    "    c = a+b\n",
    "    print(\"tracing\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'heloak'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(\"helo\"), tf.constant(\"ak\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'heloak'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(\"helo\"), tf.constant(\"ak\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发生了2件事情，\n",
    "\n",
    "第一件事情是创建计算图。\n",
    "\n",
    "即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。 在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。 主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加 tf.control_dependencies指定执行顺序依赖关系。\n",
    "\n",
    "第二件事情是执行计算图\n",
    "\n",
    "\n",
    "当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？\n",
    "只会发生一件事情，那就是上面步骤的第二步，执行计算图。\n",
    "\n",
    "由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。\n",
    "\n",
    "需要重新做2件事情：创建新的计算图、执行计算图。\n",
    "\n",
    "Note: if the data type of the argument is not Tensor in the original definition of this function, then the graph will be re-created each time after calling this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。\n",
    "\n",
    "实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。\n",
    "\n",
    "因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用tf.Module封装Autograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoModule(tf.Module):\n",
    "    def __init__(self,init_value = tf.constant(0.0),name=None):\n",
    "        super(DemoModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=True)\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "    def addprint(self,a):\n",
    "        with self.name_scope:\n",
    "            self.x.assign_add(a)\n",
    "            tf.print(self.x)\n",
    "            return(self.x)\n",
    "#执行\n",
    "demo = DemoModule(init_value = tf.constant(1.0))\n",
    "result = demo.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
