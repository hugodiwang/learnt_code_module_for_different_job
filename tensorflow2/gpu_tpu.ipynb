{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用单GPU训练模型\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#使用多GPU训练模型\n",
    "\"\"\"\n",
    "可通过以下colab链接测试效果《tf_多GPU》：\n",
    "\n",
    "https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-\n",
    "\n",
    "MirroredStrategy过程简介：\n",
    "\n",
    "训练开始前，该策略在所有 N 个计算设备上均各复制一份完整的模型；\n",
    "每次训练传入一个批次的数据时，将数据分成 N 份，分别传入 N 个计算设备（即数据并行）；\n",
    "N 个计算设备使用本地变量（镜像变量）分别计算自己所获得的部分数据的梯度；\n",
    "使用分布式计算的 All-reduce 操作，在计算设备间高效交换梯度数据并进行求和，使得最终每个设备都有了所有设备的梯度之和；\n",
    "使用梯度求和的结果更新本地变量（镜像变量）；\n",
    "当所有设备均更新本地变量后，进行下一轮训练（即该并行策略是同步的）。\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "模拟时内存设为1g 肯定不够用\n",
    "\"\"\"\n",
    "#此处在colab上使用1个GPU模拟出两个逻辑GPU进行多GPU训练\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 设置两个逻辑GPU模拟多GPU训练\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n",
    "             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "#增加以下两行代码\n",
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "with strategy.scope(): \n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)\n",
    "    \n",
    "history = model.fit(ds_train,validation_data = ds_test,epochs = 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用TPU训练模型\n",
    "#增加以下6行代码\n",
    "import os\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    model = compile_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
