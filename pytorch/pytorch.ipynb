{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "\n",
    "#进一步使用DataLoader和TensorDataset封装成可以迭代的数据管道\n",
    "dl_train = DataLoader(TensorDataset(torch.tensor(x_train).float(),torch.tensor(y_train).float()),\n",
    "                     shuffle = True, batch_size = 8)\n",
    "dl_valid = DataLoader(TensorDataset(torch.tensor(x_test).float(),torch.tensor(y_test).float()),\n",
    "                     shuffle = False, batch_size = 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add_module(\"linear1\",nn.Linear(15,20))\n",
    "    net.add_module(\"relu1\",nn.ReLU())\n",
    "    net.add_module(\"linear2\",nn.Linear(20,15))\n",
    "    net.add_module(\"relu2\",nn.ReLU())\n",
    "    net.add_module(\"linear3\",nn.Linear(15,1))\n",
    "    net.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "    return net\n",
    "    \n",
    "net = create_net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important\n",
    "from torchkeras import summary\n",
    "summary(net,input_shape=(15,))\n",
    "# 可以打印参数个数， 计算时间， 层名， shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试AdaptiveMaxPool2d的效果\n",
    "pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "t = torch.randn(10,8,32,32)\n",
    "pool(t).shape \n",
    "torch.Size([10, 8, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)\n",
    "        self.dropout = nn.Dropout2d(p = 0.1)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(64,32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(32,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        y = self.sigmoid(x)\n",
    "        return y\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchkeras\n",
    "torchkeras.summary(net,input_shape= (3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量\n",
    "        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))\n",
    "        self.conv.add_module(\"pool_1\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_1\",nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))\n",
    "        self.conv.add_module(\"pool_2\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_2\",nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\",nn.Flatten())\n",
    "        self.dense.add_module(\"linear\",nn.Linear(6144,1))\n",
    "        self.dense.add_module(\"sigmoid\",nn.Sigmoid())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "        \n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "summary(net, input_shape = (200,),input_dtype = torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import importlib \n",
    "import torchkeras \n",
    "\n",
    "torch.random.seed()\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block,self).__init__()\n",
    "    \n",
    "    def forward(self,x,x_input):\n",
    "        x_out = torch.max((1+x)*x_input[:,-1,:],torch.tensor(0.0))\n",
    "        return x_out\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 3层lstm\n",
    "        self.lstm = nn.LSTM(input_size = 3,hidden_size = 3,num_layers = 5,batch_first = True)\n",
    "        self.linear = nn.Linear(3,3)\n",
    "        self.block = Block()\n",
    "        \n",
    "    def forward(self,x_input):\n",
    "        x = self.lstm(x_input)[0][:,-1,:]\n",
    "        x = self.linear(x)\n",
    "        y = self.block(x,x_input)\n",
    "        return y\n",
    "        \n",
    "net = Net()\n",
    "model = torchkeras.Model(net)\n",
    "print(model)\n",
    "\n",
    "model.summary(input_shape=(8,3),input_dtype = torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(params=net.parameters(),lr = 0.01)\n",
    "metric_func = lambda y_pred,y_true: accuracy_score(y_true.data.numpy(),y_pred.data.numpy()>0.5)\n",
    "metric_name = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "log_step_freq = 30\n",
    "\n",
    "dfhistory = pd.DataFrame(columns = [\"epoch\",\"loss\",metric_name,\"val_loss\",\"val_\"+metric_name]) \n",
    "print(\"Start Training...\")\n",
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"==========\"*8 + \"%s\"%nowtime)\n",
    "\n",
    "for epoch in range(1,epochs+1):  \n",
    "\n",
    "    # 1，训练循环-------------------------------------------------\n",
    "    net.train()\n",
    "    loss_sum = 0.0\n",
    "    metric_sum = 0.0\n",
    "    step = 1\n",
    "    \n",
    "    for step, (features,labels) in enumerate(dl_train, 1):\n",
    "    \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 正向传播求损失\n",
    "        predictions = net(features)\n",
    "        loss = loss_func(predictions,labels)\n",
    "        metric = metric_func(predictions,labels)\n",
    "        \n",
    "        # 反向传播求梯度\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印batch级别日志\n",
    "        loss_sum += loss.item()\n",
    "        metric_sum += metric.item()\n",
    "        if step%log_step_freq == 0:   \n",
    "            print((\"[step = %d] loss: %.3f, \"+metric_name+\": %.3f\") %\n",
    "                  (step, loss_sum/step, metric_sum/step))\n",
    "            \n",
    "    # 2，验证循环-------------------------------------------------\n",
    "    net.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_metric_sum = 0.0\n",
    "    val_step = 1\n",
    "\n",
    "    for val_step, (features,labels) in enumerate(dl_valid, 1):\n",
    "        # 关闭梯度计算\n",
    "        with torch.no_grad():\n",
    "            predictions = net(features)\n",
    "            val_loss = loss_func(predictions,labels)\n",
    "            val_metric = metric_func(predictions,labels)\n",
    "        val_loss_sum += val_loss.item()\n",
    "        val_metric_sum += val_metric.item()\n",
    "\n",
    "    # 3，记录日志-------------------------------------------------\n",
    "    info = (epoch, loss_sum/step, metric_sum/step, \n",
    "            val_loss_sum/val_step, val_metric_sum/val_step)\n",
    "    dfhistory.loc[epoch-1] = info\n",
    "    \n",
    "    # 打印epoch级别日志\n",
    "    print((\"\\nEPOCH = %d, loss = %.3f,\"+ metric_name + \\\n",
    "          \"  = %.3f, val_loss = %.3f, \"+\"val_\"+ metric_name+\" = %.3f\") \n",
    "          %info)\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "        \n",
    "print('Finished Training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important callback\n",
    "\n",
    "import pytorch_lightning as pl \n",
    "from torchkeras import LightModel \n",
    "\n",
    "class Model(LightModel):\n",
    "    \n",
    "    #loss,and optional metrics\n",
    "    def shared_step(self,batch)->dict:\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = nn.BCELoss()(prediction,y)\n",
    "        preds = torch.where(prediction>0.5,torch.ones_like(prediction),torch.zeros_like(prediction))\n",
    "        acc = pl.metrics.functional.accuracy(preds, y)\n",
    "        dic = {\"loss\":loss,\"accuracy\":acc} \n",
    "        return dic\n",
    "    \n",
    "    #optimizer,and optional lr_scheduler\n",
    "    def configure_optimizers(self):\n",
    "        optimizer= torch.optim.Adagrad(self.parameters(),lr = 0.02)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "net = Net()\n",
    "model = Model(net)\n",
    "\n",
    "ckpt_cb = pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "\n",
    "# set gpus=0 will use cpu，\n",
    "# set gpus=1 will use 1 gpu\n",
    "# set gpus=2 will use 2gpus \n",
    "# set gpus = -1 will use all gpus \n",
    "# you can also set gpus = [0,1] to use the  given gpus\n",
    "# you can even set tpu_cores=2 to use two tpus \n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20,gpus = 0, callbacks=[ckpt_cb]) \n",
    "trainer.fit(model,dl_train,dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchkeras.Model(net)\n",
    "print(model)\n",
    "\n",
    "model.summary(input_shape=(8,3),input_dtype = torch.FloatTensor)\n",
    "\n",
    "def mspe(y_pred,y_true):\n",
    "    err_percent = (y_true - y_pred)**2/(torch.max(y_true**2,torch.tensor(1e-7)))\n",
    "    return torch.mean(err_percent)\n",
    "\n",
    "model.compile(loss_func = mspe,optimizer = torch.optim.Adagrad(model.parameters(),lr = 0.1))\n",
    "\n",
    "dfhistory = model.fit(100,dl_train,log_step_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true.numpy(),y_pred_cls.numpy())\n",
    "\n",
    "model.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy})\n",
    "dfhistory = model.fit(3,dl_train = dl_train, dl_val=dl_valid, log_step_freq=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model,dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = torch.cat([model.forward(t[0]) for t in dl])\n",
    "    return(result.data)\n",
    "\n",
    "y_pred_probs = predict(model,dl_valid)\n",
    "y_pred_probs\n",
    "\n",
    "# 把概率 转为 0-1\n",
    "#预测类别\n",
    "y_pred = torch.where(y_pred_probs>0.5,\n",
    "        torch.ones_like(y_pred_probs),torch.zeros_like(y_pred_probs))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(model.net.state_dict(), \"./data/model_parameter.pkl\")\n",
    "\n",
    "net_clone = Net()\n",
    "net_clone.load_state_dict(torch.load(\"./data/model_parameter.pkl\"))\n",
    "model_clone = torchkeras.Model(net_clone)\n",
    "model_clone.compile(loss_func = mspe)\n",
    "\n",
    "# 评估模型\n",
    "model_clone.evaluate(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric(dfhistory, metric):\n",
    "    train_metrics = dfhistory[metric]\n",
    "    val_metrics = dfhistory['val_'+metric]\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    plt.plot(epochs, train_metrics, 'bo--')\n",
    "    plt.plot(epochs, val_metrics, 'ro-')\n",
    "    plt.title('Training and validation '+ metric)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "    plt.show()\n",
    "plot_metric(dfhistory,\"loss\")\n",
    "plot_metric(dfhistory,\"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "\n",
    "torch.save(net.state_dict(), \"./data/net_parameter.pkl\")\n",
    "\n",
    "net_clone = create_net()\n",
    "net_clone.load_state_dict(torch.load(\"./data/net_parameter.pkl\"))\n",
    "\n",
    "net_clone.forward(torch.tensor(x_test[0:10]).float()).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image 注意事项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 导入数据\n",
    "import torchvision\n",
    "import torch\n",
    "train_augmentation = torchvision.transforms.Compose([torchvision.transforms.Resize(256),\n",
    "                                                    torchvision.transforms.RandomCrop(224),                                                                            \n",
    "                                                    torchvision.transofrms.RandomHorizontalFlip(),\n",
    "                                                    torchvision.transforms.ToTensor(),\n",
    "                                                    torch vision.Normalize([0.485, 0.456, -.406],[0.229, 0.224, 0.225])\n",
    "                                                    ])\n",
    "\n",
    "Class custom_dataread(torch.utils.data.Dataset):\n",
    "    def __init__():\n",
    "        ...\n",
    "    def __getitem__():\n",
    "        # use self.transform for input image\n",
    "    def __len__():\n",
    "        ...\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    custom_dataread(transform=train_augmentation),\n",
    "    batch_size = batch_size, shuffle = True,\n",
    "    num_workers = workers, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 导入数据\n",
    "ds_train = datasets.ImageFolder(\"./data/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "ds_valid = datasets.ImageFolder(\"./data/cifar2/test/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "\n",
    "print(ds_train.class_to_idx)\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True,num_workers=3)\n",
    "dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = True,num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示图片\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "#查看部分样本\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "for i in range(9):\n",
    "    img,label = ds_train[i]\n",
    "    img = img.permute(1,2,0)\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label.item())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text 注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext常见API一览\n",
    "\n",
    "torchtext.data.Example : 用来表示一个样本，数据和标签\n",
    "torchtext.vocab.Vocab: 词汇表，可以导入一些预训练词向量\n",
    "torchtext.data.Datasets: 数据集类，__getitem__返回 Example实例, torchtext.data.TabularDataset是其子类。\n",
    "torchtext.data.Field : 用来定义字段的处理方法（文本字段，标签字段）创建 Example时的 预处理，batch 时的一些处理操作。\n",
    "torchtext.data.Iterator: 迭代器，用来生成 batch\n",
    "torchtext.datasets: 包含了常见的数据集."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import string,re\n",
    "import torchtext\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "#分词方法\n",
    "tokenizer = lambda x:re.sub('[%s]'%string.punctuation,\"\",x).split(\" \")\n",
    "\n",
    "#过滤掉低频词\n",
    "def filterLowFreqWords(arr,vocab):\n",
    "    arr = [[x if x<MAX_WORDS else 0 for x in example] \n",
    "           for example in arr]\n",
    "    return arr\n",
    "\n",
    "#1,定义各个字段的预处理方法\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True, \n",
    "                  fix_length=MAX_LEN,postprocessing = filterLowFreqWords)\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "#2,构建表格型dataset\n",
    "#torchtext.data.TabularDataset可读取csv,tsv,json等格式\n",
    "ds_train, ds_valid = torchtext.data.TabularDataset.splits(\n",
    "        path='./data/imdb', train='train.tsv',test='test.tsv', format='tsv',\n",
    "        fields=[('label', LABEL), ('text', TEXT)],skip_header = False)\n",
    "\n",
    "#3,构建词典\n",
    "TEXT.build_vocab(ds_train)\n",
    "\n",
    "#4,构建数据管道迭代器\n",
    "train_iter, valid_iter = torchtext.data.Iterator.splits(\n",
    "        (ds_train, ds_valid),  sort_within_batch=True,sort_key=lambda x: len(x.text),\n",
    "        batch_sizes=(BATCH_SIZE,BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看example信息\n",
    "print(ds_train[0].text)\n",
    "print(ds_train[0].label)\n",
    "\n",
    "# 查看词典信息\n",
    "print(len(TEXT.vocab))\n",
    "\n",
    "#itos: index to string\n",
    "print(TEXT.vocab.itos[0]) \n",
    "print(TEXT.vocab.itos[1]) \n",
    "\n",
    "#stoi: string to index\n",
    "print(TEXT.vocab.stoi['<unk>']) #unknown 未知词\n",
    "print(TEXT.vocab.stoi['<pad>']) #padding  填充\n",
    "\n",
    "\n",
    "#freqs: 词频\n",
    "print(TEXT.vocab.freqs['<unk>']) \n",
    "print(TEXT.vocab.freqs['a']) \n",
    "print(TEXT.vocab.freqs['good']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据管道信息\n",
    "# 注意有坑：text第0维是句子长度\n",
    "for batch in train_iter:\n",
    "    features = batch.text\n",
    "    labels = batch.label\n",
    "    print(features)\n",
    "    print(features.shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 将数据管道组织成torch.utils.data.DataLoader相似的features,label输出形式\n",
    "class DataLoader:\n",
    "    def __init__(self,data_iter):\n",
    "        self.data_iter = data_iter\n",
    "        self.length = len(data_iter)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # 注意：此处调整features为 batch first，并调整label的shape和dtype\n",
    "        for batch in self.data_iter:\n",
    "            yield(torch.transpose(batch.text,0,1),\n",
    "                  torch.unsqueeze(batch.label.float(),dim = 1))\n",
    "    \n",
    "dl_train = DataLoader(train_iter)\n",
    "dl_valid = DataLoader(valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time series 注意事项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图 与 数据处理\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "df = pd.read_csv(\"./data/covid-19.csv\",sep = \"\\t\")\n",
    "df.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60);\n",
    "\n",
    "\n",
    "dfdata = df.set_index(\"date\")\n",
    "dfdiff = dfdata.diff(periods=1).dropna()\n",
    "dfdiff = dfdiff.reset_index(\"date\")\n",
    "\n",
    "dfdiff.plot(x = \"date\",y = [\"confirmed_num\",\"cured_num\",\"dead_num\"],figsize=(10,6))\n",
    "plt.xticks(rotation=60)\n",
    "dfdiff = dfdiff.drop(\"date\",axis = 1).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "\n",
    "\n",
    "#用某日前8天窗口数据作为输入预测该日数据\n",
    "WINDOW_SIZE = 8\n",
    "\n",
    "class Covid19Dataset(Dataset):\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(dfdiff) - WINDOW_SIZE\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        x = dfdiff.loc[i:i+WINDOW_SIZE-1,:]\n",
    "        feature = torch.tensor(x.values)\n",
    "        y = dfdiff.loc[i+WINDOW_SIZE,:]\n",
    "        label = torch.tensor(y.values)\n",
    "        return (feature,label)\n",
    "    \n",
    "ds_train = Covid19Dataset()\n",
    "\n",
    "#数据较小，可以将全部训练数据放入到一个batch中，提升性能\n",
    "dl_train = DataLoader(ds_train,batch_size = 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = i.type(torch.float); print(y,y.dtype) #使用type函数转换成浮点类型\n",
    "z = i.type_as(x);print(z,z.dtype) #使用type_as方法转换成某个Tensor相同类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转置操作让张量存储结构扭曲\n",
    "matrix62 = matrix26.t()\n",
    "matrix34 = matrix62.reshape(3,4) #等价于matrix34 = matrix62.contiguous().view(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# 标量求导  x 是一个值\n",
    "\n",
    "import numpy as np \n",
    "import torch \n",
    "x = torch.tensor(1.0,requires_grad = True) # x需要被求导\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "y.backward()\n",
    "dy_dx = x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标量求导  x 是多维   方法一\n",
    "\n",
    "x = torch.tensor([[0.0,0.0],[1.0,2.0]],requires_grad = True) # x需要被求导\n",
    "y = a*torch.pow(x,2) + b*x + c \n",
    "\n",
    "gradient = torch.tensor([[1.0,1.0],[1.0,1.0]]) # 初始值\n",
    "\n",
    "y.backward(gradient = gradient)\n",
    "x_grad = x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标量求导  x 是多维   方法二\n",
    "import numpy as np \n",
    "import torch \n",
    "gradient = torch.tensor([[2.0,1.0],[1.0,2.0]])\n",
    "z = torch.sum(y*gradient)\n",
    "z.backward()\n",
    "x_grad = x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用autograd 不用 backward\n",
    "\n",
    "# create_graph 设置为 True 将允许创建更高阶的导数 \n",
    "dy_dx = torch.autograd.grad(y,x,create_graph=True)[0]\n",
    "\n",
    "# 求二阶导数\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx,x)[0] \n",
    "\n",
    "# 允许同时对多个自变量求导数\n",
    "(dy1_dx1,dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)\n",
    "\n",
    "\n",
    "# 如果有多个因变量，相当于把多个因变量的梯度结果求和\n",
    "(dy12_dx1,dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用optimizer\n",
    "\n",
    "def f(x):\n",
    "    result = a*torch.pow(x,2) + b*x + c \n",
    "    return(result)\n",
    "\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态计算图\n",
    "Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。\n",
    "\n",
    "Pytorch中的计算图是动态图。这里的动态主要有两重含义。\n",
    "\n",
    "第一层含义是：计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。\n",
    "\n",
    "第二层含义是：计算图在反向传播后立即销毁。下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 重现 前向后相计算 的function \n",
    "#计算图中的另外一种节点是Function, 实际上就是 Pytorch中各种对张量操作的函数。\n",
    "#这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。\n",
    "#我们可以通过继承torch.autograd.Function来创建这种支持反向传播的Function\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "   \n",
    "    #正向传播逻辑，可以用ctx存储一些值，供反向传播使用。\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    #反向传播逻辑\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "w = torch.tensor([[3.0,1.0]],requires_grad=True)\n",
    "b = torch.tensor([[3.0]],requires_grad=True)\n",
    "X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])\n",
    "Y = torch.tensor([[2.0,3.0]])\n",
    "\n",
    "relu = MyReLU.apply # relu现在也可以具有正向传播和反向传播功能\n",
    "Y_hat = relu(X@w.t() + b)\n",
    "loss = torch.mean(torch.pow(Y_hat-Y,2))\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重要\n",
    "loss.backward()语句调用后，依次发生以下计算过程。\n",
    "\n",
    "1，loss自己的grad梯度赋值为1，即对自身的梯度为1。\n",
    "\n",
    "2，loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。\n",
    "\n",
    "3，y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。\n",
    "\n",
    "（注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述）\n",
    "\n",
    "正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重要\n",
    "四，叶子节点和非叶子节点\n",
    "执行下面代码，我们会发现 loss.grad并不是我们期望的1,而是 None。\n",
    "\n",
    "类似地 y1.grad 以及 y2.grad也是 None.\n",
    "\n",
    "这是为什么呢？这是由于它们不是叶子节点张量。\n",
    "\n",
    "在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。\n",
    "\n",
    "那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。\n",
    "\n",
    "1，叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。\n",
    "\n",
    "2，叶子节点张量的 requires_grad属性必须为True.\n",
    "\n",
    "Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。\n",
    "\n",
    "所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。\n",
    "\n",
    "如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。 如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#正向传播\n",
    "x = torch.tensor(3.0,requires_grad=True)\n",
    "y1 = x + 1\n",
    "y2 = 2*x\n",
    "loss = (y1-y2)**2\n",
    "\n",
    "#非叶子节点梯度显示控制\n",
    "y1.register_hook(lambda grad: print('y1 grad: ', grad))\n",
    "y2.register_hook(lambda grad: print('y2 grad: ', grad))\n",
    "loss.retain_grad()\n",
    "\n",
    "#反向传播\n",
    "loss.backward()\n",
    "print(\"loss.grad:\", loss.grad)\n",
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorboard\n",
    "可视化模型结构： writer.add_graph\n",
    "\n",
    "可视化指标变化： writer.add_scalar\n",
    "\n",
    "可视化参数分布： writer.add_histogram\n",
    "\n",
    "可视化原始图像： writer.add_image 或 writer.add_images\n",
    "\n",
    "可视化人工绘图： writer.add_figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bd052ae31281>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/tensorboard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_to_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# add_graph\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_graph(net,input_to_model = torch.rand(10,2))\n",
    "writer.close()\n",
    "\n",
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir ./data/tensorboard\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() \n",
    "\n",
    "#在tensorboard中查看模型\n",
    "notebook.start(\"--logdir ./data/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_scalar 看标量\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(\"x\",x.item(),i) #日志中记录x在第step i 的值\n",
    "    writer.add_scalar(\"y\",y.item(),i) #日志中记录y在第step i 的值\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_histogram  看张量\n",
    "# 创建正态分布的张量模拟参数矩阵\n",
    "def norm(mean,std):\n",
    "    t = std*torch.randn((100,20))+mean\n",
    "    return t\n",
    "\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "for step,mean in enumerate(range(-10,10,1)):\n",
    "    w = norm(mean,1)\n",
    "    writer.add_histogram(\"w\",w, step)\n",
    "    writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看图片\n",
    "#如果只写入一张图片信息，可以使用writer.add_image。\n",
    "#如果要写入多张图片信息，可以使用writer.add_images。\n",
    "#也可以用 torchvision.utils.make_grid将多张图片拼成一张图片，然后用writer.add_image写入。\n",
    "#注意，传入的是代表图片信息的Pytorch中的张量数据。\n",
    "ds_train = datasets.ImageFolder(\"./data/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "ds_valid = datasets.ImageFolder(\"./data/cifar2/test/\",\n",
    "            transform = transform_train,target_transform= lambda t:torch.tensor([t]).float())\n",
    "\n",
    "print(ds_train.class_to_idx)\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True,num_workers=3)\n",
    "dl_valid = DataLoader(ds_valid,batch_size = 50,shuffle = True,num_workers=3)\n",
    "\n",
    "dl_train_iter = iter(dl_train)\n",
    "images, labels = dl_train_iter.next()\n",
    "\n",
    "# 仅查看一张图片\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_image('images[0]', images[0])\n",
    "writer.close()\n",
    "\n",
    "# 将多张图片拼接成一张图片，中间用黑色网格分割\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('image_grid', img_grid)\n",
    "writer.close()\n",
    "\n",
    "# 将多张图片直接写入\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_images(\"images\",images,global_step = 0)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer.add_figure需要传入matplotlib的figure对象。\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "figure = plt.figure(figsize=(8,8)) \n",
    "for i in range(9):\n",
    "    img,label = ds_train[i]\n",
    "    img = img.permute(1,2,0)\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(img.numpy())\n",
    "    ax.set_title(\"label = %d\"%label.item())\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([]) \n",
    "plt.show()\n",
    "\n",
    "writer = SummaryWriter('./data/tensorboard')\n",
    "writer.add_figure('figure',figure,global_step=0)\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。\n",
    "\n",
    "######\n",
    "# important\n",
    "# 这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量\n",
    "######\n",
    "\n",
    "#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩\n",
    "q = torch.index_select(torch.index_select(scores,dim = 1,index = torch.tensor([0,5,9]))\n",
    "                   ,dim=2,index = torch.tensor([1,3,6]))\n",
    "\n",
    "#抽取第0个班级第0个学生的第0门课程，第2个班级的第4个学生的第1门课程，第3个班级的第9个学生第6门课程成绩\n",
    "#take将输入看成一维数组，输出和index同形状\n",
    "s = torch.take(scores,torch.tensor([0*10*7+0,2*10*7+4*7+1,3*10*7+9*7+6]))\n",
    "\n",
    "#抽取分数大于等于80分的分数（布尔索引）\n",
    "#结果是1维张量\n",
    "g = torch.masked_select(scores,scores>=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# important\n",
    "# 要通过修改张量的部分元素值得到新的张量，可以使用torch.where,torch.index_fill 和 torch.masked_fill\n",
    "#torch.where可以理解为if的张量版本。\n",
    "\n",
    "#torch.index_fill的选取元素逻辑和torch.index_select相同。\n",
    "\n",
    "#torch.masked_fill的选取元素逻辑和torch.masked_select相同。\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果分数大于60分，赋值成1，否则赋值成0\n",
    "ifpass = torch.where(scores>60,torch.tensor(1),torch.tensor(0))\n",
    "\n",
    "#将每个班级第0个学生，第5个学生，第9个学生的全部成绩赋值成满分\n",
    "torch.index_fill(scores,dim = 1,index = torch.tensor([0,5,9]),value = 100)\n",
    "#等价于 scores.index_fill(dim = 1,index = torch.tensor([0,5,9]),value = 100)\n",
    "\n",
    "#将分数小于60分的分数赋值成60分\n",
    "b = torch.masked_fill(scores,scores<60,60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cum扫描\n",
    "a = torch.arange(1,10)\n",
    "\n",
    "print(torch.cumsum(a,0))\n",
    "print(torch.cumprod(a,0))\n",
    "print(torch.cummax(a,0).values)\n",
    "print(torch.cummax(a,0).indices)\n",
    "print(torch.cummin(a,0))\n",
    "\n",
    "#torch.sort和torch.topk可以对张量排序\n",
    "u,s,v = torch.svd(a)\n",
    "print(torch.eig(a,eigenvectors=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播机制\n",
    "Pytorch的广播规则和numpy是一样的:\n",
    "\n",
    "1、如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。\n",
    "2、如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。\n",
    "3、如果两个张量在所有维度上都是相容的，它们就能使用广播。\n",
    "4、广播之后，每个维度的长度将取两个张量在该维度长度的较大值。\n",
    "5、在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。\n",
    "torch.broadcast_tensors可以将多个张量根据广播规则转换成相同的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_broad,b_broad = torch.broadcast_tensors(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pytorch一般将参数用nn.Parameter来表示，并且用nn.Module来管理其结构下的所有参数。\n",
    "\n",
    "# nn.Parameter 具有 requires_grad = True 属性\n",
    "\n",
    "# nn.ParameterList 可以将多个nn.Parameter组成一个列表\n",
    "params_list = nn.ParameterList([nn.Parameter(torch.rand(8,i)) for i in range(1,3)])\n",
    "\n",
    "# nn.ParameterDict 可以将多个nn.Parameter组成一个字典\n",
    "params_dict = nn.ParameterDict({\"a\":nn.Parameter(torch.rand(2,2)),\n",
    "                               \"b\":nn.Parameter(torch.zeros(2))})\n",
    "\n",
    "# 可以用Module将它们管理起来\n",
    "# module.parameters()返回一个生成器，包括其结构下的所有parameters\n",
    "for param in module.parameters():\n",
    "    print(param,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实践当中，一般通过继承nn.Module来构建模块类，并将所有含有需要学习的参数的部分放在构造函数中。\n",
    "\n",
    "#以下范例为Pytorch中nn.Linear的源码的简化版本\n",
    "#可以看到它将需要学习的参数放在了__init__构造函数中，并在forward中调用F.linear函数来实现计算逻辑。\n",
    "class Linear(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.Tensor(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下，我们都很少直接使用 nn.Parameter来定义参数构建模型，而是通过一些拼装一些常用的模型层来构造模型。\n",
    "\n",
    "这些模型层也是继承自nn.Module的对象,本身也包括参数，属于我们要定义的模块的子模块。\n",
    "\n",
    "nn.Module提供了一些方法可以管理这些子模块。\n",
    "\n",
    "children() 方法: 返回生成器，包括模块下的所有子模块。\n",
    "\n",
    "named_children()方法：返回一个生成器，包括模块下的所有子模块，以及它们的名字。\n",
    "\n",
    "modules()方法：返回一个生成器，包括模块下的所有各个层级的模块，包括模块本身。\n",
    "\n",
    "named_modules()方法：返回一个生成器，包括模块下的所有各个层级的模块以及它们的名字，包括模块本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in net.children():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet DataLoader\n",
    "在绝大部分情况下，用户只需实现Dataset的__len__方法和__getitem__方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer\n",
    "nn.Linear：全连接层。参数个数 = 输入层特征数× 输出层特征数(weight)＋ 输出层特征数(bias)\n",
    "\n",
    "nn.Flatten：压平层，用于将多维张量样本压成一维张量样本。\n",
    "\n",
    "nn.BatchNorm1d：一维批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。可以用afine参数设置该层是否含有可以训练的参数。\n",
    "\n",
    "nn.BatchNorm2d：二维批标准化层。\n",
    "\n",
    "nn.BatchNorm3d：三维批标准化层。\n",
    "\n",
    "nn.Dropout：一维随机丢弃层。一种正则化手段。\n",
    "\n",
    "nn.Dropout2d：二维随机丢弃层。\n",
    "\n",
    "nn.Dropout3d：三维随机丢弃层。\n",
    "\n",
    "nn.Threshold：限幅层。当输入大于或小于阈值范围时，截断之。\n",
    "\n",
    "nn.ConstantPad2d： 二维常数填充层。对二维张量样本填充常数扩展长度。\n",
    "\n",
    "nn.ReplicationPad1d： 一维复制填充层。对一维张量样本通过复制边缘值填充扩展长度。\n",
    "\n",
    "nn.ZeroPad2d：二维零值填充层。对二维张量样本在边缘填充0值.\n",
    "\n",
    "nn.GroupNorm：组归一化。一种替代批归一化的方法，将通道分成若干组进行归一。不受batch大小限制，据称性能和效果都优于BatchNorm。\n",
    "\n",
    "nn.LayerNorm：层归一化。较少使用。\n",
    "\n",
    "nn.InstanceNorm2d: 样本归一化。较少使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Conv1d：普通一维卷积，常用于文本。参数个数 = 输入通道数×卷积核尺寸(如3)×卷积核个数 + 卷积核尺寸(如3）\n",
    "\n",
    "nn.Conv2d：普通二维卷积，常用于图像。参数个数 = 输入通道数×卷积核尺寸(如3乘3)×卷积核个数 + 卷积核尺寸(如3乘3) 通过调整dilation参数大于1，可以变成空洞卷积，增大卷积核感受野。 通过调整groups参数不为1，可以变成分组卷积。分组卷积中不同分组使用相同的卷积核，显著减少参数数量。 当groups参数等于通道数时，相当于tensorflow中的二维深度卷积层tf.keras.layers.DepthwiseConv2D。 利用分组卷积和1乘1卷积的组合操作，可以构造相当于Keras中的二维深度可分离卷积层tf.keras.layers.SeparableConv2D。\n",
    "\n",
    "nn.Conv3d：普通三维卷积，常用于视频。参数个数 = 输入通道数×卷积核尺寸(如3乘3乘3)×卷积核个数 + 卷积核尺寸(如3乘3乘3) 。\n",
    "\n",
    "nn.MaxPool1d: 一维最大池化。\n",
    "\n",
    "nn.MaxPool2d：二维最大池化。一种下采样方式。没有需要训练的参数。\n",
    "\n",
    "nn.MaxPool3d：三维最大池化。\n",
    "\n",
    "nn.AdaptiveMaxPool2d：二维自适应最大池化。无论输入图像的尺寸如何变化，输出的图像尺寸是固定的。 该函数的实现原理，大概是通过输入图像的尺寸和要得到的输出图像的尺寸来反向推算池化算子的padding,stride等参数。\n",
    "\n",
    "nn.FractionalMaxPool2d：二维分数最大池化。普通最大池化通常输入尺寸是输出的整数倍。而分数最大池化则可以不必是整数。分数最大池化使用了一些随机采样策略，有一定的正则效果，可以用它来代替普通最大池化和Dropout层。\n",
    "\n",
    "nn.AvgPool2d：二维平均池化。\n",
    "\n",
    "nn.AdaptiveAvgPool2d：二维自适应平均池化。无论输入的维度如何变化，输出的维度是固定的。\n",
    "\n",
    "nn.ConvTranspose2d：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。在语义分割中可用于上采样。\n",
    "\n",
    "nn.Upsample：上采样层，操作效果和池化相反。可以通过mode参数控制上采样策略为\"nearest\"最邻近策略或\"linear\"线性插值策略。\n",
    "\n",
    "nn.Unfold：滑动窗口提取层。其参数和卷积操作nn.Conv2d相同。实际上，卷积操作可以等价于nn.Unfold和nn.Linear以及nn.Fold的一个组合。 其中nn.Unfold操作可以从输入中提取各个滑动窗口的数值矩阵，并将其压平成一维。利用nn.Linear将nn.Unfold的输出和卷积核做乘法后，再使用 nn.Fold操作将结果转换成输出图片形状。\n",
    "\n",
    "nn.Fold：逆滑动窗口提取层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding：嵌入层。一种比Onehot更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。\n",
    "\n",
    "nn.LSTM：长短记忆循环网络层【支持多层】。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置bidirectional = True时可以得到双向LSTM。需要注意的时，默认的输入和输出形状是(seq,batch,feature), 如果需要将batch维度放在第0维，则要设置batch_first参数设置为True。\n",
    "\n",
    "nn.GRU：门控循环网络层【支持多层】。LSTM的低配版，不具有携带轨道，参数数量少于LSTM，训练速度更快。\n",
    "\n",
    "nn.RNN：简单循环网络层【支持多层】。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。\n",
    "\n",
    "nn.LSTMCell：长短记忆循环网络单元。和nn.LSTM在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。\n",
    "\n",
    "nn.GRUCell：门控循环网络单元。和nn.GRU在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。\n",
    "\n",
    "nn.RNNCell：简单循环网络单元。和nn.RNN在整个序列上迭代相比，它仅在序列上迭代一步。一般较少使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Transformer：Transformer网络结构。Transformer网络结构是替代循环网络的一种结构，解决了循环网络难以并行，难以捕捉长期依赖的缺陷。它是目前NLP任务的主流模型的主要构成部分。Transformer网络结构由TransformerEncoder编码器和TransformerDecoder解码器组成。编码器和解码器的核心是MultiheadAttention多头注意力层。\n",
    "\n",
    "nn.TransformerEncoder：Transformer编码器结构。由多个 nn.TransformerEncoderLayer编码器层组成。\n",
    "\n",
    "nn.TransformerDecoder：Transformer解码器结构。由多个 nn.TransformerDecoderLayer解码器层组成。\n",
    "\n",
    "nn.TransformerEncoderLayer：Transformer的编码器层。\n",
    "\n",
    "nn.TransformerDecoderLayer：Transformer的解码器层。\n",
    "\n",
    "nn.MultiheadAttention：多头注意力层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要使用多个GPU训练模型，也非常简单。只需要在将模型设置为数据并行风格模型。 则模型移动到GPU上之后，会在每一个GPU上拷贝一个副本，并把数据平分到各个GPU上进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "... \n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model) # 包装为并行风格模型\n",
    "\n",
    "# 训练模型\n",
    "...\n",
    "features = features.to(device) # 移动数据到cuda\n",
    "labels = labels.to(device) # 或者 labels = labels.cuda() if torch.cuda.is_available() else labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意保存参数时要指定保存model.module的参数\n",
    "torch.save(model.module.state_dict(), \"./data/model_parameter.pkl\") \n",
    "\n",
    "linear = nn.Linear(2,1)\n",
    "linear.load_state_dict(torch.load(\"./data/model_parameter.pkl\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5，清空cuda缓存\n",
    "\n",
    "# 该方法在cuda超内存时十分有用\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true.cpu().numpy(),y_pred_cls.cpu().numpy()) \n",
    "    # 注意此处要将数据先移动到cpu上，然后才能转换成numpy数组\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy},device = device) # 注意此处compile时指定了device\n",
    "\n",
    "dfhistory = model.fit(3,dl_train = dl_train, dl_val=dl_valid, log_step_freq=100)  # 只用在 model上处理， input不用处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model parameters\n",
    "torch.save(model.net.module.state_dict(), \"model_parameter.pkl\")\n",
    "\n",
    "net_clone = CnnModel()\n",
    "net_clone.load_state_dict(torch.load(\"model_parameter.pkl\"))\n",
    "\n",
    "model_clone = torchkeras.Model(net_clone)\n",
    "model_clone.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy},device = device)\n",
    "model_clone.evaluate(dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "\n",
    "# set gpus=0 will use cpu，\n",
    "# set gpus=1 will use 1 gpu\n",
    "# set gpus=2 will use 2gpus \n",
    "# set gpus = -1 will use all gpus \n",
    "# you can also set gpus = [0,1] to use the  given gpus\n",
    "# you can even set tpu_cores=2 to use two tpus \n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10,gpus = 2, callbacks=[ckpt_cb]) \n",
    "\n",
    "trainer.fit(model,dl_train,dl_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
